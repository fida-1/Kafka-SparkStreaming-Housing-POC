# ğŸš€ Pipeline Kafka-Spark Streaming â†’ PostgreSQL

Un pipeline d'ingestion de donnÃ©es en temps rÃ©el qui :
1. **Lit** des microbatches d'un CSV via Java Producer
2. **Envoie** les donnÃ©es Ã  Kafka
3. **Traite** les streams avec Spark Streaming
4. **Stocke** les rÃ©sultats dans PostgreSQL

---

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   housing   â”‚
â”‚   .csv      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Java Producer (100 records)    â”‚
â”‚  - Batch processing             â”‚
â”‚  - JSON serialization           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Kafka      â”‚
        â”‚  Topic: data   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚
      â”Œâ”€â”€â–¼â”€â”€â”         â”Œâ”€â”€â”€â–¼â”€â”€â”
      â”‚ UI  â”‚         â”‚Spark â”‚
      â”‚8082 â”‚         â”‚8080  â”‚
      â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”¬â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Spark Streaming Job    â”‚
        â”‚  - Parse JSON              â”‚
        â”‚  - Flatten schema          â”‚
        â”‚  - Type casting            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ PostgreSQL  â”‚
                   â”‚   housing   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  PgAdmin    â”‚
                   â”‚    5050     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ PrÃ©requis

- **Docker** et **Docker Compose** installÃ©s
- **Java 11+** et **Maven** (pour le Producer)
- **Python 3.8+** (dans les conteneurs Docker)
- **Git Bash** ou **PowerShell** pour exÃ©cuter les scripts

---

## ğŸ“¦ Fichiers du Projet

```
â”œâ”€â”€ docker-compose.yml          # Configuration de tous les services
â”œâ”€â”€ init.sql                    # SchÃ©ma PostgreSQL
â”œâ”€â”€ create_topic.sh             # Script crÃ©ation du topic Kafka
â”œâ”€â”€ submit_consumer.sh           # Script soumission du job Spark
â”œâ”€â”€ test_pipeline.sh            # Tests automatisÃ©s (Bash)
â”œâ”€â”€ quick_start.ps1             # Quick start interactif (PowerShell)
â”œâ”€â”€ GUIDE_EXECUTION.md          # Guide complet
â”œâ”€â”€ producer/                   # Java Producer
â”‚   â”œâ”€â”€ pom.xml                # DÃ©pendances Maven
â”‚   â”œâ”€â”€ src/main/java/...      # Code source
â”‚   â””â”€â”€ housing.csv            # DonnÃ©es source
â”œâ”€â”€ consumer/                   # Spark Streaming Consumer
â”‚   â””â”€â”€ consumer.py            # Code PySpark
â””â”€â”€ data/
    â””â”€â”€ housing.csv            # Dataset (Boston Housing)
```

---

## ğŸš€ DÃ©marrage Rapide (Windows PowerShell)

### Option 1ï¸âƒ£ : Menu Interactif (RecommandÃ©)

```powershell
# ExÃ©cuter le script interactif
powershell -ExecutionPolicy Bypass -File quick_start.ps1

# Ensuite, suivez le menu pour :
# 1. DÃ©marrer les services Docker
# 2. CrÃ©er le topic Kafka
# 3. Compiler le Producer
# 4. ExÃ©cuter le Producer
# 5. Soumettre le job Spark
```

### Option 2ï¸âƒ£ : Commandes Manuelles

#### Ã‰tape 1: DÃ©marrer Docker
```powershell
docker-compose up -d
Start-Sleep -Seconds 20
```

#### Ã‰tape 2: CrÃ©er le Topic Kafka
```powershell
docker exec kafka kafka-topics --create `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --partitions 1 `
  --replication-factor 1 `
  --if-not-exists
```

#### Ã‰tape 3: Compiler et ExÃ©cuter le Producer
```powershell
cd producer
mvn clean package
mvn exec:java@default
```

#### Ã‰tape 4: Soumettre le Consumer Spark (Terminal 2)
```powershell
bash submit_consumer.sh
```

#### Ã‰tape 5: VÃ©rifier les donnÃ©es (Terminal 3)
```powershell
# Attendre 30-60 secondes que les donnÃ©es arrivent dans PostgreSQL
docker exec -it postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

---

## ğŸš€ DÃ©marrage Rapide (Linux/Mac)

```bash
# DÃ©marrer les services
docker-compose up -d
sleep 20

# CrÃ©er le topic
bash create_topic.sh

# Terminal 1: Producer
cd producer && mvn clean package && mvn exec:java@default

# Terminal 2: Consumer
bash submit_consumer.sh

# Terminal 3: VÃ©rifier les donnÃ©es
docker exec -it postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

---

## ğŸŒ Web Interfaces

Ouvrez ces URLs dans votre navigateur :

| Service | URL | Credentials |
|---------|-----|-------------|
| **Kafka UI** | http://localhost:8082 | N/A |
| **Spark Master** | http://localhost:8080 | N/A |
| **Spark Worker** | http://localhost:8081 | N/A |
| **PgAdmin** | http://localhost:5050 | admin@example.com / admin |

### Actions dans chaque interface

#### ğŸ”µ Kafka UI (8082)
- Voir le topic `housing-data`
- Visualiser les messages en temps rÃ©el
- Voir les partitions et offsets
- Monitorer les producers/consumers

#### ğŸŸ  Spark Master (8080)
- Applications en cours d'exÃ©cution
- Cluster Overview
- Voir l'application Spark Streaming
- Cliquer sur l'app pour voir les dÃ©tails
- VÃ©rifier les executor status

#### ğŸ˜ PgAdmin (5050)
1. Se connecter: `admin@example.com` / `admin`
2. Ajouter serveur :
   - Hostname: `postgres`
   - Port: `5432`
   - User: `kafka_user`
   - Password: `kafka_pass`
   - Database: `kafka_streaming`
3. Naviguer: `Servers â†’ postgres â†’ Databases â†’ kafka_streaming â†’ Schemas â†’ public â†’ Tables â†’ housing`
4. Clic droit "View All Rows" pour voir les donnÃ©es

---

## âœ… VÃ©rifications de l'ExÃ©cution

### 1ï¸âƒ£ VÃ©rifier que les services sont actifs
```powershell
docker ps
# Doit afficher: zookeeper, kafka, postgres, spark-master, spark-worker, kafka-ui, pgadmin
```

### 2ï¸âƒ£ VÃ©rifier le topic Kafka
```powershell
docker exec kafka kafka-topics --describe --topic housing-data --bootstrap-server localhost:9092
```

### 3ï¸âƒ£ VÃ©rifier les messages Kafka
```powershell
docker exec kafka kafka-console-consumer `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --from-beginning `
  --max-messages 2
```

### 4ï¸âƒ£ VÃ©rifier les donnÃ©es PostgreSQL
```powershell
# Nombre total
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"

# Voir les 10 premiers records
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT * FROM housing LIMIT 10;"

# Statistiques
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*), AVG(medv), MIN(medv), MAX(medv) FROM housing;"
```

### 5ï¸âƒ£ VÃ©rifier le job Spark
```powershell
# Logs du conteneur Spark
docker logs -f spark-master | tail -20

# AccÃ©der au conteneur
docker exec -it spark-master bash
ls /opt/spark/work-dir/
cat consumer.py
```

---

## ğŸ”„ Flux de DonnÃ©es Complet

```
1. CSV (506 records Boston Housing)
   â†“
2. Java Producer lit le CSV par microbatches (100 records)
   â†“
3. SÃ©rialize en JSON et envoie Ã  Kafka
   â†“
4. Kafka topic "housing-data" reÃ§oit les messages
   â†“
5. Spark Streaming Consumer s'abonne au topic
   â†“
6. Parse le JSON, dÃ©tecte le type de donnÃ©es
   â†“
7. Cast les types (float, int, etc.)
   â†“
8. Ã‰crit par batch dans PostgreSQL
   â†“
9. Table housing contient les donnÃ©es persistantes
```

---

## ğŸ§ª Test AutomatisÃ©

### Bash/Linux/Mac
```bash
bash test_pipeline.sh
```

### PowerShell (Windows)
```powershell
powershell -ExecutionPolicy Bypass -File quick_start.ps1
# Puis choisir option 8 (Run all tests)
```

---

## ğŸ›‘ ArrÃªter le Pipeline

### ArrÃªter les containers (donnÃ©es persistantes)
```powershell
docker-compose stop
```

### Supprimer les containers
```powershell
docker-compose down
```

### Supprimer complÃ¨tement (donnÃ©es incluses)
```powershell
docker-compose down -v
```

---

## âš ï¸ Troubleshooting

### âŒ "Topic does not exist"
```powershell
bash create_topic.sh
```

### âŒ "Connection refused to kafka:29092"
- Attendre 20-30 secondes aprÃ¨s `docker-compose up -d`
- VÃ©rifier que le conteneur kafka est actif: `docker logs kafka`

### âŒ "Can't connect to postgres"
- VÃ©rifier que postgres est prÃªt: `docker logs postgres`
- Test manuel: `docker exec -it postgres psql -U kafka_user`

### âŒ "Spark job stuck / no data in PostgreSQL"
```powershell
# Voir les logs Spark
docker logs spark-master

# VÃ©rifier que le job s'exÃ©cute
docker ps | grep spark

# RedÃ©marrer le job
bash submit_consumer.sh
```

### âŒ "Producer stuck on sending"
- VÃ©rifier Kafka: `docker logs kafka | tail -20`
- VÃ©rifier le fichier CSV existe: `ls producer/housing.csv`
- RedÃ©marrer: `docker-compose restart kafka`

### âŒ "psycopg2 not found in Spark"
- Le script `submit_consumer.sh` installe automatiquement psycopg2
- Si encore erreur: `docker exec spark-master pip install --upgrade psycopg2-binary`

### âŒ "OutOfMemory in Spark"
- Augmenter la mÃ©moire des conteneurs dans `docker-compose.yml`
- Ajouter Ã  `spark-master`:
```yaml
environment:
  SPARK_DRIVER_MEMORY: 2g
  SPARK_EXECUTOR_MEMORY: 1g
```

---

## ğŸ“‹ Logs et Debugging

### Voir les logs en temps rÃ©el
```powershell
docker logs -f kafka          # Kafka
docker logs -f spark-master   # Spark
docker logs -f postgres       # PostgreSQL
```

### AccÃ©der aux conteneurs
```powershell
docker exec -it spark-master bash
docker exec -it postgres bash
docker exec -it kafka bash
```

### VÃ©rifier les ressources
```powershell
docker stats
```

---

## ğŸ“Š SchÃ©ma PostgreSQL

```sql
CREATE TABLE housing (
    id SERIAL PRIMARY KEY,
    crim DOUBLE PRECISION,
    zn DOUBLE PRECISION,
    indus DOUBLE PRECISION,
    chas INTEGER,
    nox DOUBLE PRECISION,
    rm DOUBLE PRECISION,
    age DOUBLE PRECISION,
    dis DOUBLE PRECISION,
    rad INTEGER,
    tax INTEGER,
    ptratio DOUBLE PRECISION,
    b DOUBLE PRECISION,
    lstat DOUBLE PRECISION,
    medv DOUBLE PRECISION,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Colonnes
- **crim**: Crime rate per capita
- **zn**: Proportion of residential land
- **indus**: Proportion of industrial business
- **chas**: Charles River dummy variable
- **nox**: Nitrogen oxides concentration
- **rm**: Average number of rooms
- **age**: Proportion of buildings built before 1940
- **dis**: Distance to employment centers
- **rad**: Index of accessibility to radial highways
- **tax**: Property tax rate
- **ptratio**: Pupil-teacher ratio by town
- **b**: 1000(B - 0.63)^2 where B is the proportion of blacks
- **lstat**: Percentage lower status of the population
- **medv**: Median value of homes in $1000s

---

## ğŸ¯ RÃ©sultats Attendus

### Producer Output
```
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records of Kafka
Sent batch of 6 records to Kafka
```

### PostgreSQL Final
```
count
-----
 506

(1 row)
```

### Spark Streaming (visible dans http://localhost:8080)
- Status: RUNNING
- Uptime: variable
- Records processed: 506

---

## ğŸ“ Configuration

### Batch Size (Producer)
```java
private static final int BATCH_SIZE = 100; // Dans KafkaProducerApp.java
```

### Kafka Partition
```yaml
--partitions 1      # Dans create_topic.sh
```

### PostgreSQL Connection
```python
conn = psycopg2.connect(
    host="postgres",
    port=5432,
    database="kafka_streaming",
    user="kafka_user",
    password="kafka_pass"
)
```

---

## ğŸ“š Ressources Utiles

- **Kafka Documentation**: https://kafka.apache.org/documentation/
- **Spark Streaming**: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
- **PostgreSQL JDBC**: https://jdbc.postgresql.org/
- **Docker Compose**: https://docs.docker.com/compose/

---

## ğŸ¤ Support

Si vous rencontrez des problÃ¨mes :

1. VÃ©rifiez les **logs** : `docker logs <service_name>`
2. Consultez le **GUIDE_EXECUTION.md** pour des dÃ©tails
3. Testez avec **test_pipeline.sh** ou **quick_start.ps1**
4. VÃ©rifiez les **Web Interfaces** pour le statut en temps rÃ©el

---

## âœ¨ Prochaines Ã‰tapes

- Augmenter le batch size pour plus de donnÃ©es
- Ajouter des transformations Spark supplÃ©mentaires
- ImplÃ©menter des requÃªtes analytiques PostgreSQL
- Ajouter de la monitoring (Prometheus, Grafana)
- Mettre en place des alertes Kafka

