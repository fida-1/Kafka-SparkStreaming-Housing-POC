# âœ… CONFIGURATION FINALE - VÃ‰RIFICATION PRÃ‰ALABLE

## ğŸ¯ Avant de Lancer: Checklist

### 1ï¸âƒ£ VÃ©rifier Docker
```powershell
docker --version
docker-compose --version
```
âœ… **Attendu:** Docker version 20.10+ et Docker Compose 2.0+

### 2ï¸âƒ£ VÃ©rifier Java
```powershell
java -version
```
âœ… **Attendu:** Java 11 ou plus

### 3ï¸âƒ£ VÃ©rifier Maven
```powershell
mvn --version
```
âœ… **Attendu:** Maven 3.6.0+

### 4ï¸âƒ£ VÃ©rifier le Dossier du Projet
```powershell
Test-Path "C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming"
```
âœ… **Attendu:** `True`

### 5ï¸âƒ£ VÃ©rifier les Fichiers Critiques
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming
Test-Path "docker-compose.yml"
Test-Path "producer/pom.xml"
Test-Path "producer/housing.csv"
Test-Path "consumer/consumer.py"
Test-Path "init.sql"
```
âœ… **Attendu:** Tous `True`

### 6ï¸âƒ£ VÃ©rifier les Ports Disponibles
```powershell
# VÃ©rifier que les ports ne sont pas utilisÃ©s
netstat -ano | findstr :9092     # Kafka
netstat -ano | findstr :5432     # PostgreSQL
netstat -ano | findstr :8080     # Spark
netstat -ano | findstr :5050     # PgAdmin
```
âœ… **Attendu:** Pas de rÃ©sultats (ports libres)

### 7ï¸âƒ£ VÃ©rifier l'Espace Disque
```powershell
# Au moins 5 GB libre pour les images Docker
Get-Volume C: | Select-Object SizeRemaining
```
âœ… **Attendu:** SizeRemaining > 5 GB

---

## ğŸ³ Configuration Docker

### docker-compose.yml (VÃ©rifications)

```yaml
# âœ… Services (7 total)
services:
  - zookeeper         âœ…
  - kafka            âœ…
  - postgres         âœ…
  - spark-master     âœ…
  - spark-worker     âœ…
  - kafka-ui         âœ…
  - pgadmin          âœ…

# âœ… Ports
  zookeeper: 2181    âœ…
  kafka: 9092        âœ…
  postgres: 5432     âœ…
  spark: 8080, 7077  âœ…
  kafka-ui: 8082     âœ…
  pgadmin: 5050      âœ…
```

### Fichiers SQL (init.sql)

âœ… Table `housing` crÃ©Ã©e
âœ… Colonne `id SERIAL PRIMARY KEY`
âœ… Colonne `created_at TIMESTAMP`
âœ… Indexes sur `medv` et `created_at`

### Fichiers de Configuration

âœ… `producer/pom.xml` - DÃ©pendances Maven OK
âœ… `consumer/consumer.py` - Code Spark OK
âœ… `producer/housing.csv` - DonnÃ©es prÃ©sentes (506 records)

---

## ğŸ“ Scripts de DÃ©marrage

### create_topic.sh âœ…
```bash
docker exec kafka kafka-topics --create \
  --topic housing-data \
  --bootstrap-server localhost:9092 \
  --partitions 1 \
  --replication-factor 1
```

### submit_consumer.sh âœ…
```bash
#!/bin/bash
echo "Waiting for Spark cluster to be ready..."
sleep 15

docker cp consumer/consumer.py spark-master:/opt/spark/work-dir/consumer.py
docker exec spark-master pip install psycopg2-binary > /dev/null 2>&1
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.postgresql:postgresql:42.6.0 \
  /opt/spark/work-dir/consumer.py
```

---

## â˜• Code Java (KafkaProducerApp.java)

âœ… Lecture du CSV
âœ… Microbatches de 100 records
âœ… SÃ©rialisation JSON
âœ… Envoi Ã  Kafka

```java
private static final int BATCH_SIZE = 100;  // âœ… Correct
private static final String TOPIC = "housing-data";  // âœ… Correct
```

---

## ğŸ Code Spark (consumer.py)

âœ… Lecture depuis Kafka
âœ… Parsing JSON
âœ… Cast des types
âœ… Ã‰criture dans PostgreSQL

```python
# âœ… Connexion PostgreSQL correcte
conn = psycopg2.connect(
    host="postgres",  # âœ… Nom du container
    port=5432,
    database="kafka_streaming",
    user="kafka_user",
    password="kafka_pass"
)
```

---

## ğŸ“Š Variables d'Environnement Docker

### PostgreSQL
```yaml
POSTGRES_DB: kafka_streaming        âœ…
POSTGRES_USER: kafka_user           âœ…
POSTGRES_PASSWORD: kafka_pass       âœ…
```

### Kafka
```yaml
KAFKA_BROKER_ID: 1                  âœ…
KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  âœ…
KAFKA_ADVERTISED_LISTENERS: 
  PLAINTEXT://kafka:29092,
  PLAINTEXT_HOST://localhost:9092   âœ…
```

### Spark
```yaml
SPARK_MODE: master / worker         âœ…
SPARK_RPC_AUTHENTICATION_ENABLED: no  âœ…
```

---

## ğŸ”— ConnectivitÃ© (Intra-Docker)

| Component | Internal Name | Port | Protocol |
|-----------|--------------|------|----------|
| Kafka | kafka | 29092 | PLAINTEXT |
| PostgreSQL | postgres | 5432 | psycopg2 |
| Spark Master | spark-master | 7077 | Spark Protocol |
| Zookeeper | zookeeper | 2181 | Zookeeper |

âœ… **Tous les noms de host correspondent aux noms de container**

---

## ğŸŒ ConnectivitÃ© (Externe - Localhost)

| Service | URL | Port | AccÃ¨s |
|---------|-----|------|-------|
| Kafka | localhost | 9092 | âœ… Producer |
| Kafka UI | localhost:8082 | 8082 | âœ… Browser |
| Spark Master | localhost:8080 | 8080 | âœ… Browser |
| Spark Worker | localhost:8081 | 8081 | âœ… Browser |
| PgAdmin | localhost:5050 | 5050 | âœ… Browser |

---

## ğŸ” Credentials VÃ©rifiÃ©s

### PostgreSQL
- User: `kafka_user` âœ…
- Password: `kafka_pass` âœ…
- Database: `kafka_streaming` âœ…
- Host: `postgres` âœ…
- Port: `5432` âœ…

### PgAdmin
- Email: `admin@example.com` âœ…
- Password: `admin` âœ…

### Kafka (pas de credentials)
- Bootstrap: `localhost:9092` âœ…
- Internal: `kafka:29092` âœ…

---

## ğŸ“¦ DÃ©pendances VÃ©rifiÃ©es

### Maven (pom.xml)
```xml
âœ… org.apache.kafka:kafka-clients:3.4.0
âœ… com.fasterxml.jackson.core:jackson-databind:2.15.2
```

### Spark Packages (--packages)
```
âœ… org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1
âœ… org.postgresql:postgresql:42.6.0
```

### Python (pip)
```
âœ… psycopg2-binary (installÃ© automatiquement)
```

---

## ğŸ§ª Tests PrÃ©alables

### Test 1: Docker fonctionne
```powershell
docker run --rm hello-world
```
âœ… **Attendu:** "Hello from Docker!"

### Test 2: Les ports sont libres
```powershell
# Kafka
Test-NetConnection -ComputerName localhost -Port 9092
# PostgreSQL
Test-NetConnection -ComputerName localhost -Port 5432
```
âœ… **Attendu:** `TcpTestSucceeded : False` (port libre)

### Test 3: Java compile
```powershell
cd producer
mvn compile
```
âœ… **Attendu:** Pas d'erreurs

### Test 4: CSV existe
```powershell
(Get-Item "producer/housing.csv").Length -gt 0
```
âœ… **Attendu:** `True`

---

## âš™ï¸ Configurations PersonnalisÃ©es (Optionnel)

### Augmenter Batch Size
**Fichier:** `producer/src/main/java/com/example/KafkaProducerApp.java`
```java
// De:
private static final int BATCH_SIZE = 100;
// Ã€:
private static final int BATCH_SIZE = 500;
```
Puis: `mvn clean package`

### Augmenter Partitions Kafka
**Fichier:** `create_topic.sh`
```bash
# De:
--partitions 1
# Ã€:
--partitions 4
```
Puis: `bash create_topic.sh`

### Augmenter MÃ©moire Spark
**Fichier:** `docker-compose.yml` (spark-master et spark-worker)
```yaml
environment:
  SPARK_DRIVER_MEMORY: 4g
  SPARK_EXECUTOR_MEMORY: 2g
```
Puis: `docker-compose up -d`

### Changer Credentials PostgreSQL
**Fichier:** `docker-compose.yml` (postgres)
```yaml
environment:
  POSTGRES_PASSWORD: your_new_password
```
âœ… **Attention:** Aussi Ã  jour dans init.sql si nÃ©cessaire

---

## ğŸ“‹ RÃ©sumÃ© des Fichiers ModifiÃ©s

| Fichier | Modification | âœ… |
|---------|-------------|-----|
| submit_consumer.sh | ExÃ©cution DANS le container | âœ… |
| init.sql | ID primaire + timestamp | âœ… |
| docker-compose.yml | PgAdmin password ajoutÃ© | âœ… |

---

## ğŸš¨ Points Critiques Ã  VÃ©rifier

âœ… **Ordre d'exÃ©cution:** Terminal 1 â†’ 2 â†’ 3 â†’ 4
âœ… **Attendre:** 20 sec aprÃ¨s docker-compose up -d
âœ… **Attendre:** 60 sec aprÃ¨s que le Producer ait fini
âœ… **Soumettre Spark:** AVANT de lancer le Producer
âœ… **Ports:** Tous libres et accessibles
âœ… **CSV:** Existe et contient 506 records
âœ… **Credentials:** Corrects dans tous les fichiers

---

## âœ… PRÃŠT Ã€ LANCER!

Si tous les points ci-dessus sont vÃ©rifiÃ©s âœ…, vous pouvez:

```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming
# Puis ouvrir 4 terminaux et suivre QUICK_START_5MIN.md ou TOUTES_LES_COMMANDES.md
```

---

## ğŸ†˜ Si Quelque Chose N'est Pas OK

1. **Docker ne marche pas?**
   - RedÃ©marrer Docker Desktop
   - VÃ©rifier: `docker ps`

2. **Maven timeout?**
   - Attendre plus longtemps
   - Ou: `mvn clean package -T 1`

3. **Port dÃ©jÃ  utilisÃ©?**
   - Terminer le processus: `netstat -ano | findstr :PORT`
   - Puis: `taskkill /PID <PID> /F`

4. **CSV manquant?**
   - TÃ©lÃ©charger Boston Housing dataset
   - Placer dans: `producer/housing.csv`

5. **Credentials incorrects?**
   - VÃ©rifier docker-compose.yml
   - VÃ©rifier init.sql
   - VÃ©rifier KafkaProducerApp.java
   - VÃ©rifier consumer.py

---

## ğŸ‰ Configuration ValidÃ©e!

Vous Ãªtes prÃªt Ã  exÃ©cuter le pipeline complet!

**Prochaine Ã©tape:** Lisez `QUICK_START_5MIN.md` ou `TOUTES_LES_COMMANDES.md`

