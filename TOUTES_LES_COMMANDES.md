# üéØ TOUTES LES COMMANDES - ORDRE D'EX√âCUTION COMPLET

## ‚ö° R√âSUM√â RAPIDE (Copier-Coller Facile)

### Terminal 1: Infrastructure (2-3 minutes)
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming

docker-compose up -d

Start-Sleep -Seconds 20

docker exec kafka kafka-topics --create --topic housing-data --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists

cd producer && mvn clean package && cd ..

Write-Host "‚úÖ Infrastructure OK! Allez au Terminal 2" -ForegroundColor Green
```

---

### Terminal 2: Spark Consumer (1 minute)
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming

bash submit_consumer.sh

Write-Host "‚úÖ Spark Job soumis!" -ForegroundColor Green
```

---

### Terminal 3: Producer Java (30 secondes)
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming\producer

mvn exec:java@default

# Attendez que √ßa se termine (6 batches envoy√©s)
```

---

### Terminal 4: V√©rification (1 minute)
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming

# Attendre 30-60 secondes apr√®s que Terminal 3 ait fini

docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"

# R√©sultat attendu: 506 ‚úÖ
```

---

## üìã D√âTAIL COMPLET - LIGNE PAR LIGNE

### üü¢ PHASE 1: INFRASTRUCTURE DOCKER

#### 1.1 Naviguer au dossier projet
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming
```

#### 1.2 D√©marrer tous les services Docker
```powershell
docker-compose up -d
```
**R√©sultat attendu:**
```
Creating network "kafka-sparktreaming_default" with the default driver
Creating zookeeper ... done
Creating kafka ... done
Creating postgres ... done
Creating spark-master ... done
Creating spark-worker ... done
Creating kafka-ui ... done
Creating pgadmin ... done
```

#### 1.3 Attendre que les services soient pr√™ts
```powershell
Start-Sleep -Seconds 20
```

#### 1.4 V√©rifier que tous les 7 services tournent
```powershell
docker ps
```
**R√©sultat attendu:** 7 containers (zookeeper, kafka, postgres, spark-master, spark-worker, kafka-ui, pgadmin)

#### 1.5 Cr√©er le topic Kafka
```powershell
docker exec kafka kafka-topics --create `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --partitions 1 `
  --replication-factor 1 `
  --if-not-exists
```
**R√©sultat attendu:**
```
Created topic housing-data.
```

#### 1.6 V√©rifier que le topic a √©t√© cr√©√©
```powershell
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```
**R√©sultat attendu:**
```
housing-data
```

#### 1.7 Naviguer au dossier producer
```powershell
cd producer
```

#### 1.8 Compiler le Producer Java (nettoyer les anciens builds)
```powershell
mvn clean
```
**Attendre:** ~30 secondes

#### 1.9 Compiler le code source
```powershell
mvn compile
```
**Attendre:** ~30 secondes

#### 1.10 Packager en JAR
```powershell
mvn package
```
**Attendre:** ~1-2 minutes
**R√©sultat attendu:**
```
BUILD SUCCESS
```

#### 1.11 Retourner au dossier racine
```powershell
cd ..
```

**√âtape 1 TERMIN√âE ‚úÖ Allez maintenant au Terminal 2**

---

### üü¢ PHASE 2: SOUMETTRE LE JOB SPARK (Terminal 2)

#### 2.1 Naviguer au dossier projet
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming
```

#### 2.2 Soumettre le job Spark Streaming
```powershell
bash submit_consumer.sh
```
**R√©sultat attendu:**
```
Waiting for Spark cluster to be ready...
Copying consumer.py to Spark container...
Installing psycopg2-binary...
Submitting Spark Streaming job...
```

#### 2.3 V√©rifier que le job a √©t√© soumis
```powershell
docker logs spark-master | tail -20
```
**R√©sultat attendu:**
```
Submitted application app-XXX
```

**√âtape 2 TERMIN√âE ‚úÖ Allez maintenant au Terminal 3**

---

### üü¢ PHASE 3: EX√âCUTER LE PRODUCER (Terminal 3)

#### 3.1 Naviguer au dossier producer
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming\producer
```

#### 3.2 Ex√©cuter le Producer Java
```powershell
mvn exec:java@default
```

**R√©sultat attendu (plusieurs fois):**
```
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 6 records to Kafka
```

**L'ex√©cution dure:** ~30-60 secondes

**√âtape 3 TERMIN√âE ‚úÖ Une fois que le Producer a fini, allez au Terminal 4**

---

### üü¢ PHASE 4: V√âRIFICATION (Terminal 4)

#### 4.1 Naviguer au dossier projet
```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming
```

#### 4.2 Attendre que les donn√©es arrivent dans PostgreSQL
```powershell
Start-Sleep -Seconds 60
```
**(Important: Attendre 60 secondes pour que Spark traite et √©crive les donn√©es)**

#### 4.3 ‚úÖ V√©rifier le nombre total de records
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```
**R√©sultat attendu:**
```
 count
-------
   506
(1 row)
```

#### 4.4 Voir les premiers records
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT * FROM housing LIMIT 5;"
```
**R√©sultat attendu:** 5 lignes avec les colonnes du housing dataset

#### 4.5 Voir les statistiques d√©taill√©es
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*), AVG(medv)::numeric(5,2) as avg_price, MIN(medv) as min_price, MAX(medv) as max_price FROM housing;"
```
**R√©sultat attendu:**
```
 count | avg_price | min_price | max_price
-------+-----------+-----------+-----------
   506 |     22.53 |       5.0 |      50.0
```

#### 4.6 Voir les donn√©es les plus r√©centes
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT *, created_at FROM housing ORDER BY created_at DESC LIMIT 5;"
```
**R√©sultat attendu:** 5 derniers records avec timestamp

---

## üåê √âTAPE 5: OUVRIR LES WEB INTERFACES

#### 5.1 Ouvrir Kafka UI dans le navigateur
```powershell
Start-Process "http://localhost:8082"
```
**√Ä faire:** Voir le topic `housing-data` avec 6 messages

#### 5.2 Ouvrir Spark Master UI
```powershell
Start-Process "http://localhost:8080"
```
**√Ä faire:** Voir l'application Spark RUNNING

#### 5.3 Ouvrir Spark Worker UI
```powershell
Start-Process "http://localhost:8081"
```
**√Ä faire:** Voir les ressources utilis√©es

#### 5.4 Ouvrir PgAdmin
```powershell
Start-Process "http://localhost:5050"
```
**√Ä faire:**
1. Connectez-vous: `admin@example.com` / `admin`
2. Cliquez sur "Servers" ‚Üí Ajouter serveur
3. Remplissez:
   - Hostname: `postgres`
   - Port: `5432`
   - User: `kafka_user`
   - Password: `kafka_pass`
   - Database: `kafka_streaming`
4. Naviguez vers la table `housing`
5. Cliquez "View All Rows"

---

## üß™ √âTAPE 6: TESTS ET V√âRIFICATIONS SUPPL√âMENTAIRES

### Test 1: V√©rifier les messages Kafka
```powershell
docker exec kafka kafka-console-consumer `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --from-beginning `
  --max-messages 1
```

### Test 2: V√©rifier le descriptif du topic
```powershell
docker exec kafka kafka-topics --describe `
  --topic housing-data `
  --bootstrap-server localhost:9092
```

### Test 3: V√©rifier la connexion PostgreSQL
```powershell
docker exec -it postgres psql -U kafka_user -d kafka_streaming

# √Ä l'int√©rieur de psql, tapez:
SELECT * FROM housing WHERE id = 1;
\q  # Pour quitter
```

### Test 4: Voir tous les records
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

### Test 5: V√©rifier les logs Spark
```powershell
docker logs -f spark-master | tail -50
```

### Test 6: V√©rifier que Spark a bien √©crit les donn√©es
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT * FROM housing WHERE created_at IS NOT NULL ORDER BY created_at DESC LIMIT 10;"
```

---

## üõë √âTAPE 7: ARR√äT PROPRE DU PIPELINE

### Option 1: Arr√™ter temporairement (garder les donn√©es)
```powershell
docker-compose stop
```

### Option 2: Arr√™ter et supprimer les containers (garder les donn√©es)
```powershell
docker-compose down
```

### Option 3: Suppression COMPL√àTE y compris les donn√©es
```powershell
docker-compose down -v
```

---

## üìä COMMANDES DE MONITORING CONTINU

### Voir les logs en temps r√©el
```powershell
# Kafka
docker logs -f kafka

# Spark Master
docker logs -f spark-master

# PostgreSQL
docker logs -f postgres

# Tous les logs
docker-compose logs -f
```

### Voir les ressources utilis√©es
```powershell
docker stats

# Avec refresh continu
docker stats --no-stream
```

### Voir le statut de tous les containers
```powershell
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

---

## üîÑ √âTAPE 8: REPRISE APR√àS ARR√äT

### Si vous avez arr√™t√© avec `docker-compose stop`
```powershell
docker-compose start

# Attendre 10 secondes
Start-Sleep -Seconds 10

# V√©rifier les donn√©es sont toujours l√†
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

### Si vous avez supprim√© avec `docker-compose down -v`
**Les donn√©es sont perdues, recommencez depuis le Terminal 1**

---

## üéØ CHECKLIST COMPL√àTE

- [ ] Terminal 1: Infrastructure Docker d√©marr√©e
- [ ] Terminal 1: Topic Kafka cr√©√©
- [ ] Terminal 1: Producer compil√© (mvn clean package)
- [ ] Terminal 1: TERMIN√â ‚úÖ

- [ ] Terminal 2: Spark job soumis (bash submit_consumer.sh)
- [ ] Terminal 2: TERMIN√â ‚úÖ

- [ ] Terminal 3: Producer ex√©cut√© (mvn exec:java@default)
- [ ] Terminal 3: 6 batches envoy√©s
- [ ] Terminal 3: TERMIN√â ‚úÖ

- [ ] Terminal 4: Attendre 60 secondes
- [ ] Terminal 4: 506 records v√©rifi√©s dans PostgreSQL ‚úÖ
- [ ] Terminal 4: Statistiques correctes ‚úÖ
- [ ] Terminal 4: TERMIN√â ‚úÖ

- [ ] Kafka UI (8082): Messages visibles ‚úÖ
- [ ] Spark UI (8080): Application RUNNING ‚úÖ
- [ ] PgAdmin (5050): Donn√©es visibles ‚úÖ

---

## ‚ö†Ô∏è TROUBLESHOOTING PAR COMMANDE

### Si mvn clean package √©choue
```powershell
# Nettoyer compl√®tement
mvn clean -X

# V√©rifier Java version
java -version

# R√©essayer
mvn clean package
```

### Si Docker containers ne d√©marrent pas
```powershell
# V√©rifier Docker
docker ps

# Red√©marrer Docker Desktop
# Puis:
docker-compose up -d --force-recreate
```

### Si Kafka topic ne se cr√©e pas
```powershell
# V√©rifier Kafka est pr√™t
docker logs kafka | tail -20

# Attendre puis r√©essayer
Start-Sleep -Seconds 30
bash create_topic.sh
```

### Si Spark job ne s'ex√©cute pas
```powershell
# Voir les logs
docker logs spark-master

# Red√©marrer Spark
docker-compose restart spark-master spark-worker

# Resubmit
bash submit_consumer.sh
```

### Si PostgreSQL a 0 records
```powershell
# V√©rifier que Spark a √©crit
docker logs spark-master | grep -i "write"

# V√©rifier que la table existe
docker exec postgres psql -U kafka_user -d kafka_streaming -c "\dt"

# R√©initialiser la table
docker exec postgres psql -U kafka_user -d kafka_streaming -c "TRUNCATE housing;"

# Recommencer depuis Terminal 2
```

---

## üöÄ COMMANDE UNIQUE ONE-LINER (Avec D√©lais)

Pour ex√©cuter tout d'un coup dans un seul terminal (pas recommand√©, mais fonctionne):

```powershell
cd C:\Users\khamm\OneDrive\Bureau\Kafka-SparkTreaming; `
docker-compose up -d; `
Start-Sleep -Seconds 20; `
docker exec kafka kafka-topics --create --topic housing-data --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists; `
cd producer; mvn clean package; cd ..; `
bash submit_consumer.sh; `
Start-Sleep -Seconds 5; `
cd producer; mvn exec:java@default; `
Start-Sleep -Seconds 60; `
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

---

## üìà TEMPS TOTAL

| √âtape | Temps |
|-------|-------|
| Terminal 1 (Infrastructure) | 2-3 minutes |
| Terminal 2 (Spark) | 1 minute |
| Terminal 3 (Producer) | 30-60 secondes |
| Terminal 4 (V√©rification) | 2 minutes (dont 1 min d'attente) |
| **TOTAL** | **~5-7 minutes** |

---

## ‚úÖ SUCC√àS = CES 4 SIGNES

1. **Terminal 3 affiche:** `Sent batch of 6 records to Kafka`
2. **Terminal 4 affiche:** `506` records dans PostgreSQL
3. **Kafka UI (8082):** Voit le topic avec 6 messages
4. **PgAdmin (5050):** Affiche 506 rows dans la table housing

Si vous voyez ces 4 points = **SUCC√àS COMPLET ‚úÖ**

