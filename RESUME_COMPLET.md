# üìã R√âSUM√â COMPLET - Projet Kafka Spark Streaming ‚Üí PostgreSQL

## üéØ Objectif du Projet

Cr√©er un **pipeline d'ingestion de donn√©es en temps r√©el** qui:
1. ‚úÖ Lit des microbatches d'un fichier CSV (Boston Housing: 506 records)
2. ‚úÖ Les envoie √† **Kafka** en tant que messages JSON
3. ‚úÖ **Spark Streaming** les consomme et traite en temps r√©el
4. ‚úÖ Les stock finalement dans **PostgreSQL**

---

## üîß Corrections Apport√©es

### ‚ùå Probl√®me 1: Script `submit_consumer.sh` incorrect
**Avant:**
```bash
docker exec spark-master sh -c "pip install psycopg2-binary"
/opt/spark/bin/spark-submit \  # ‚ö†Ô∏è Ex√©cut√© HORS du conteneur!
```

**Apr√®s:**
```bash
docker exec spark-master pip install psycopg2-binary > /dev/null 2>&1
docker exec spark-master /opt/spark/bin/spark-submit \  # ‚úÖ Ex√©cut√© DANS le conteneur
```

### ‚ùå Probl√®me 2: Table PostgreSQL sans cl√© primaire
**Avant:**
```sql
CREATE TABLE housing (
    crim DOUBLE PRECISION,
    -- ... pas de cl√© primaire
);
```

**Apr√®s:**
```sql
CREATE TABLE housing (
    id SERIAL PRIMARY KEY,  -- ‚úÖ Cl√© primaire ajout√©e
    -- ... colonnes
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP  -- ‚úÖ Timestamp ajout√©
);
```

### ‚ùå Probl√®me 3: Manque de gestion des timeouts
**Avant:** Pas d'attente entre les √©tapes

**Apr√®s:**
```bash
sleep 15  # ‚úÖ Attendez que Spark soit pr√™t
docker exec spark-master pip install psycopg2-binary > /dev/null 2>&1  # ‚úÖ Installation silencieuse
```

---

## üìä Architecture D√©taill√©e

```
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ                    KAFKA-SPARK STREAMING PIPELINE                  ‚îÉ
‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DONN√âES SOURCE                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  housing.csv (506 records, 14 colonnes)                             ‚îÇ
‚îÇ  - CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO,   ‚îÇ
‚îÇ    B, LSTAT, MEDV                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PHASE 1: PRODUCER (Java)                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  KafkaProducerApp.java                                              ‚îÇ
‚îÇ  - Lit le CSV ligne par ligne                                       ‚îÇ
‚îÇ  - Accumule 100 records dans un batch                               ‚îÇ
‚îÇ  - S√©rialize en JSON                                                ‚îÇ
‚îÇ  - Envoie √† Kafka                                                   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  R√©sultat: 6 batches (100+100+100+100+100+6)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  KAFKA BROKER       ‚îÇ
    ‚îÇ  Topic: housing-    ‚îÇ
    ‚îÇ         data        ‚îÇ
    ‚îÇ  Partitions: 1      ‚îÇ
    ‚îÇ  Messages: 6        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ          ‚îÇ
         ‚ñº          ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Kafka   ‚îÇ  ‚îÇ  Spark   ‚îÇ
    ‚îÇ  UI     ‚îÇ  ‚îÇ Consumer ‚îÇ
    ‚îÇ :8082   ‚îÇ  ‚îÇ Job      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           PHASE 2: SPARK STREAMING CONSUMER (Python)               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  consumer.py                                                        ‚îÇ
‚îÇ  - S'abonne au topic Kafka                                          ‚îÇ
‚îÇ  - Parse les messages JSON (ArrayType de records)                   ‚îÇ
‚îÇ  - Explode les records                                              ‚îÇ
‚îÇ  - Cast les types (float, int, etc.)                                ‚îÇ
‚îÇ  - Ex√©cute foreachBatch                                             ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  R√©sultat: DataFrames transform√©es                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                           ‚îÇ
         ‚ñº                           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Spark   ‚îÇ             ‚îÇ  PgSQL       ‚îÇ
    ‚îÇ Master   ‚îÇ             ‚îÇ  Write       ‚îÇ
    ‚îÇ  :8080   ‚îÇ             ‚îÇ  Query       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PHASE 3: STOCKAGE (PostgreSQL)                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Table: housing                                                      ‚îÇ
‚îÇ  - id (SERIAL PRIMARY KEY)                                          ‚îÇ
‚îÇ  - 14 colonnes DOUBLE PRECISION / INTEGER                           ‚îÇ
‚îÇ  - created_at (TIMESTAMP DEFAULT CURRENT_TIMESTAMP)                 ‚îÇ
‚îÇ  - Indexes sur: medv, created_at                                    ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  R√©sultat: 506 records persistants                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ
         ‚ñº               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ PgAdmin  ‚îÇ   ‚îÇ Requ√™tes  ‚îÇ
    ‚îÇ  :5050   ‚îÇ   ‚îÇ   SQL     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Services Docker Utilis√©s

| Service | Image | Port | R√¥le |
|---------|-------|------|------|
| **Zookeeper** | confluentinc/cp-zookeeper:7.4.0 | 2181 | Coordination Kafka |
| **Kafka** | confluentinc/cp-kafka:7.4.0 | 9092 | Message Broker |
| **PostgreSQL** | postgres:15 | 5432 | Base de donn√©es |
| **Spark Master** | apache/spark:3.4.1 | 8080 | Orchestration Spark |
| **Spark Worker** | apache/spark:3.4.1 | 8081 | Ex√©cution des jobs |
| **Kafka UI** | provectuslabs/kafka-ui | 8082 | Monitoring Kafka |
| **PgAdmin** | dpage/pgadmin4 | 5050 | Management PostgreSQL |

---

## üìã Fichiers du Projet (Cr√©√©s/Modifi√©s)

### üìÅ Structure Compl√®te
```
Kafka-SparkTreaming/
‚îú‚îÄ‚îÄ docker-compose.yml              # ‚úÖ Configuration Docker
‚îú‚îÄ‚îÄ init.sql                        # ‚úÖ Sch√©ma PostgreSQL (modifi√©)
‚îú‚îÄ‚îÄ create_topic.sh                 # Topic Kafka
‚îú‚îÄ‚îÄ submit_consumer.sh              # ‚úÖ Job Spark (CORRIG√â)
‚îÇ
‚îú‚îÄ‚îÄ README_COMPLET.md               # üìñ Guide complet
‚îú‚îÄ‚îÄ GUIDE_EXECUTION.md              # üìñ Guide d√©taill√© d'ex√©cution
‚îú‚îÄ‚îÄ COMMANDES_PRINCIPALES.md        # üìñ Commandes cl√©s
‚îÇ
‚îú‚îÄ‚îÄ quick_start.ps1                 # üöÄ Script interactif (PowerShell)
‚îú‚îÄ‚îÄ test_pipeline.sh                # üß™ Tests automatis√©s (Bash)
‚îÇ
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml                     # Maven configuration
‚îÇ   ‚îú‚îÄ‚îÄ src/main/java/com/example/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ KafkaProducerApp.java   # Java Producer
‚îÇ   ‚îî‚îÄ‚îÄ housing.csv                 # Donn√©es source
‚îÇ
‚îú‚îÄ‚îÄ consumer/
‚îÇ   ‚îî‚îÄ‚îÄ consumer.py                 # PySpark Consumer
‚îÇ
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ housing.csv                 # Dataset Boston Housing
```

---

## ‚è±Ô∏è Ordre d'Ex√©cution Recommand√©

### 1Ô∏è‚É£ Terminal 1: D√©marrer l'Infrastructure
```powershell
docker-compose up -d
Start-Sleep -Seconds 20
docker exec kafka kafka-topics --create --topic housing-data `
  --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists
cd producer && mvn clean package
```

### 2Ô∏è‚É£ Terminal 2: Soumettre Spark Streaming
```powershell
bash submit_consumer.sh
```

### 3Ô∏è‚É£ Terminal 3: Ex√©cuter le Producer
```powershell
cd producer
mvn exec:java@default
```

### 4Ô∏è‚É£ Terminal 4: V√©rifier les R√©sultats
```powershell
# Attendre 30-60 secondes
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

---

## üåê Web Interfaces de Monitoring

| Nom | URL | Login | Fonction |
|-----|-----|-------|----------|
| **Kafka UI** | http://localhost:8082 | - | Voir les messages Kafka en temps r√©el |
| **Spark Master** | http://localhost:8080 | - | Voir l'application Spark et les statuts |
| **Spark Worker** | http://localhost:8081 | - | Voir les ressources utilis√©es |
| **PgAdmin** | http://localhost:5050 | admin@example.com / admin | G√©rer PostgreSQL et voir les donn√©es |

---

## ‚úÖ Points de Contr√¥le Cl√©s

### 1. Infrastructure Ready
```powershell
docker ps
# ‚úÖ 7 containers actifs
```

### 2. Kafka Topic Cr√©√©
```powershell
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
# ‚úÖ "housing-data" dans la liste
```

### 3. Spark Job Soumis
```powershell
docker logs spark-master | grep "Submitted application"
# ‚úÖ Application soumise
```

### 4. Donn√©es dans PostgreSQL
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
# ‚úÖ count = 506
```

### 5. Web Interfaces Accessibles
```
‚úÖ Kafka UI: voir les 6 messages
‚úÖ Spark UI: voir l'application RUNNING
‚úÖ PgAdmin: voir 506 records
```

---

## üß™ Commandes de Test

```powershell
# Test 1: Docker services
docker ps | Measure-Object -Line  # Doit √™tre 7

# Test 2: Kafka messages
docker exec kafka kafka-console-consumer `
  --topic housing-data --bootstrap-server localhost:9092 `
  --from-beginning --max-messages 1

# Test 3: PostgreSQL data
docker exec postgres psql -U kafka_user -d kafka_streaming `
  -c "SELECT COUNT(*) FROM housing;"

# Test 4: Spark status
docker exec spark-master curl -s http://localhost:8080/json

# Test 5: Database stats
docker exec postgres psql -U kafka_user -d kafka_streaming `
  -c "SELECT COUNT(*), AVG(medv), MIN(medv), MAX(medv) FROM housing;"
```

---

## ‚ùå Probl√®mes Courants & Solutions

| Probl√®me | Solution |
|----------|----------|
| **Topic does not exist** | `bash create_topic.sh` |
| **Connection refused kafka** | `Start-Sleep -Seconds 30; docker logs kafka` |
| **psycopg2 not found** | `docker exec spark-master pip install psycopg2-binary` |
| **No data in PostgreSQL** | `docker logs spark-master` et `bash submit_consumer.sh` |
| **Spark job stuck** | `docker exec spark-master pkill -f spark-submit` puis red√©marrer |
| **Maven timeout** | Attendre ou augmenter timeout dans pom.xml |
| **Out of memory** | Augmenter SPARK_DRIVER_MEMORY et SPARK_EXECUTOR_MEMORY |

---

## üìä R√©sultats Attendus

### Output Producer
```
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 100 records to Kafka
Sent batch of 6 records to Kafka
```

### Output Spark (logs)
```
Submitting Spark Streaming job...
20XX-XX-XX XX:XX:XX INFO AppStatusListener: Registered BlockManagerMaster
20XX-XX-XX XX:XX:XX INFO BlockManagerMasterEndpoint: BlockManagerMaster started
```

### Output PostgreSQL
```
 count
-------
   506
(1 row)
```

---

## üîê Credentials et Configuration

| Composant | Credential | Valeur |
|-----------|------------|--------|
| **PostgreSQL** | User | `kafka_user` |
| **PostgreSQL** | Password | `kafka_pass` |
| **PostgreSQL** | Database | `kafka_streaming` |
| **PgAdmin** | Email | `admin@example.com` |
| **PgAdmin** | Password | `admin` |
| **Kafka** | Bootstrap Server | `localhost:9092` |
| **Kafka (interne)** | - | `kafka:29092` |
| **Spark Master** | Host | `spark-master` |
| **Spark Master** | Port | `7077` |

---

## üéØ Checklist Finale

- [ ] Docker-compose up -d ‚úÖ
- [ ] Tous les 7 services running ‚úÖ
- [ ] Topic Kafka cr√©√© ‚úÖ
- [ ] Producer compil√© (mvn clean package) ‚úÖ
- [ ] Spark job soumis (bash submit_consumer.sh) ‚úÖ
- [ ] Producer ex√©cut√© (mvn exec:java@default) ‚úÖ
- [ ] Kafka UI accessible et voit les messages ‚úÖ
- [ ] Spark UI accessible et application RUNNING ‚úÖ
- [ ] PostgreSQL contient 506 records ‚úÖ
- [ ] PgAdmin accessible et donn√©es visibles ‚úÖ

---

## üöÄ Quick Start PowerShell (Copier-Coller)

```powershell
# Phase 1: Infrastructure
docker-compose up -d
Start-Sleep -Seconds 20
docker exec kafka kafka-topics --create --topic housing-data --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists
cd producer
mvn clean package

# Phase 2 (nouveau terminal): Spark
bash submit_consumer.sh

# Phase 3 (nouveau terminal): Producer
cd producer
mvn exec:java@default

# Phase 4 (nouveau terminal): V√©rification
Start-Sleep -Seconds 30
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"

# Ouvrir les interfaces web
# Kafka UI: http://localhost:8082
# Spark: http://localhost:8080
# PgAdmin: http://localhost:5050
```

---

## üìö Ressources Cr√©√©es

### Documentation
- üìñ **README_COMPLET.md** - Guide complet du projet
- üìñ **GUIDE_EXECUTION.md** - Instructions d√©taill√©es
- üìñ **COMMANDES_PRINCIPALES.md** - Commandes cl√©s et debugging
- üìã **Ce fichier** - R√©sum√© complet

### Scripts Utiles
- üöÄ **quick_start.ps1** - Menu interactif PowerShell
- üß™ **test_pipeline.sh** - Tests automatis√©s Bash
- ‚úÖ **submit_consumer.sh** - Job Spark (CORRIG√â)

### Configurations
- ‚öôÔ∏è **docker-compose.yml** - Services Docker (am√©lior√©)
- üìù **init.sql** - Sch√©ma PostgreSQL (am√©lior√©)

---

## üéì Prochaines √âtapes (Am√©liorations Possibles)

1. **Augmenter les donn√©es**
   - Batch size: 100 ‚Üí 500
   - Partitions Kafka: 1 ‚Üí 4

2. **Ajouter de la r√©silience**
   - Replication factor: 1 ‚Üí 3
   - Monitoring: Prometheus + Grafana

3. **Optimiser les performances**
   - Augmenter la m√©moire Spark
   - Ajouter des indexes PostgreSQL suppl√©mentaires

4. **Ajouter de la s√©curit√©**
   - TLS/SSL pour Kafka
   - Authentication Kafka SASL
   - Chiffrer les mots de passe

5. **Monitoring & Alertes**
   - Prometheus pour les m√©triques
   - Grafana pour les dashboards
   - Alertes bas√©es sur les seuils

---

## üí° Tips & Tricks

**Sauvegarde rapide des donn√©es:**
```powershell
docker exec postgres pg_dump -U kafka_user kafka_streaming > backup.sql
```

**Nettoyer compl√®tement (y compris donn√©es):**
```powershell
docker-compose down -v
```

**Voir les logs en temps r√©el:**
```powershell
docker logs -f spark-master
docker logs -f kafka
docker logs -f postgres
```

**Red√©marrer un service:**
```powershell
docker-compose restart spark-master
```

