# üöÄ COMMANDES PRINCIPALES - KAFKA SPARK STREAMING

## ‚è±Ô∏è ORDRE D'EX√âCUTION COMPLET

### Phase 1: Infrastructure (Terminal 1)
```powershell
# 1a. D√©marrer Docker
docker-compose up -d

# 1b. Attendre 20 secondes
Start-Sleep -Seconds 20

# 1c. Cr√©er le topic Kafka
docker exec kafka kafka-topics --create `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --partitions 1 `
  --replication-factor 1 `
  --if-not-exists

# 1d. V√©rifier que le topic est cr√©√©
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

---

### Phase 2: Construire le Producer (Terminal 1)
```powershell
cd producer
mvn clean package
# Attendez que le build soit termin√© (env. 30-60 secondes)
cd ..
```

---

### Phase 3: Soumettre le Job Spark (Terminal 2 - AVANT de lancer le Producer)
```powershell
# IMPORTANT: Faire √ßa avant de lancer le Producer!
bash submit_consumer.sh

# Attendez de voir des messages comme:
# "Waiting for Spark cluster to be ready..."
# "Submitting Spark Streaming job..."
```

---

### Phase 4: Ex√©cuter le Producer (Terminal 1 ou 3)
```powershell
cd producer
mvn exec:java@default

# Vous verrez:
# Sent batch of 100 records to Kafka
# Sent batch of 100 records to Kafka
# ... (plusieurs fois)
# Sent batch of 6 records to Kafka
```

---

### Phase 5: V√©rifier les R√©sultats (Terminal 3 ou 4)

#### 5a. V√©rifier le nombre de records en PostgreSQL
```powershell
# Attendre 30-60 secondes apr√®s que le Producer ait fini
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"

# R√©sultat attendu:
#  count
# -------
#    506
# (1 row)
```

#### 5b. Voir les 10 premiers records
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT * FROM housing LIMIT 10;"
```

#### 5c. Voir les statistiques
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*), AVG(medv), MIN(medv), MAX(medv) FROM housing;"
```

#### 5d. Voir les donn√©es les plus r√©centes
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT *, created_at FROM housing ORDER BY created_at DESC LIMIT 5;"
```

---

## üåê OUVRIR LES WEB INTERFACES

### Dans votre navigateur:

```
Kafka UI:        http://localhost:8082
Spark Master:    http://localhost:8080
Spark Worker:    http://localhost:8081
PgAdmin:         http://localhost:5050
```

### Actions dans chaque interface:

**Kafka UI (8082):**
- Voir les messages du topic `housing-data`
- Voir les partitions
- Voir les offsets

**Spark Master (8080):**
- Voir l'application en cours
- Voir les statuts des executors
- Cliquer sur l'application pour les d√©tails

**Spark Worker (8081):**
- Voir les ressources utilis√©es
- Voir les executors actifs

**PgAdmin (5050):**
- Login: `admin@example.com` / `admin`
- Ajouter le serveur PostgreSQL
- Voir les donn√©es en temps r√©el

---

## üîç COMMANDES DE MONITORING/DEBUGGING

### V√©rifier l'√©tat des services
```powershell
# Tous les containers
docker ps

# Logs Kafka
docker logs -f kafka

# Logs Spark
docker logs -f spark-master

# Logs PostgreSQL
docker logs -f postgres

# Stats en temps r√©el
docker stats
```

---

### V√©rifier les donn√©es √† chaque √©tape

#### Dans Kafka
```powershell
# Voir les messages du topic (les 2 premiers)
docker exec kafka kafka-console-consumer `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --from-beginning `
  --max-messages 2

# Voir les stats du topic
docker exec kafka kafka-topics --describe `
  --topic housing-data `
  --bootstrap-server localhost:9092
```

#### Dans PostgreSQL
```powershell
# Connexion interactive
docker exec -it postgres psql -U kafka_user -d kafka_streaming

# Puis √† l'int√©rieur de psql:
SELECT COUNT(*) FROM housing;
SELECT * FROM housing LIMIT 5;
SELECT COUNT(*), AVG(medv), MIN(medv), MAX(medv) FROM housing;
\dt  # Voir les tables
\d housing  # Voir le sch√©ma de la table
```

---

### Acc√©der aux conteneurs

```powershell
# Terminal dans Spark
docker exec -it spark-master bash

# Terminal dans Kafka
docker exec -it kafka bash

# Terminal dans PostgreSQL
docker exec -it postgres bash
```

---

## ‚öôÔ∏è CONFIGURATION & PERSONALISATION

### Changer la taille du batch (Producer)
**Fichier:** `producer/src/main/java/com/example/KafkaProducerApp.java`
```java
private static final int BATCH_SIZE = 100;  // Changer cette valeur
```

### Changer le nombre de partitions Kafka
**Fichier:** `create_topic.sh`
```bash
--partitions 1  # Changer cette valeur
```

### Changer les credentials PostgreSQL
**Fichier:** `docker-compose.yml`
```yaml
environment:
  POSTGRES_DB: kafka_streaming
  POSTGRES_USER: kafka_user
  POSTGRES_PASSWORD: kafka_pass  # Changer le mot de passe
```

---

## üõë ARR√äT ET NETTOYAGE

```powershell
# Arr√™ter les containers (garder les donn√©es)
docker-compose stop

# Arr√™ter et supprimer les containers
docker-compose down

# Arr√™ter, supprimer et EFFACER les donn√©es
docker-compose down -v

# Voir le statut apr√®s
docker ps
```

---

## üß™ TESTS RAPIDES

### Test 1: Les services Docker tournent-ils?
```powershell
docker ps | Measure-Object -Line  # Doit afficher 7 containers
```

### Test 2: Kafka est-il ready?
```powershell
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Test 3: PostgreSQL est-il ready?
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT 1"
```

### Test 4: Le topic a des messages?
```powershell
docker exec kafka kafka-console-consumer `
  --topic housing-data `
  --bootstrap-server localhost:9092 `
  --from-beginning `
  --max-messages 1 `
  --timeout-ms 5000
```

### Test 5: Les donn√©es arrivent dans PostgreSQL?
```powershell
docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

---

## üìä R√âSULTATS ATTENDUS

### Avant d'ex√©cuter le Producer
```
count
-------
    0
(1 row)
```

### Apr√®s avoir ex√©cut√© le Producer et Spark Streaming
```
count
-------
  506
(1 row)
```

### Avec les statistiques
```
count | avg(medv) | min(medv) | max(medv)
------+-----------+-----------+----------
  506 |  22.53    |   5.0     |   50.0
(1 row)
```

---

## ‚ö†Ô∏è PROBL√àMES COURANTS

### Probl√®me: "Topic does not exist"
**Solution:**
```powershell
bash create_topic.sh
```

### Probl√®me: "Connection refused to kafka"
**Solution:**
```powershell
# Attendre 20-30 secondes apr√®s docker-compose up -d
docker logs kafka
```

### Probl√®me: "psycopg2 not found"
**Solution:**
```powershell
docker exec spark-master pip install psycopg2-binary
```

### Probl√®me: "No data in PostgreSQL after 5 minutes"
**Solution:**
```powershell
# V√©rifier les logs Spark
docker logs spark-master | tail -50

# Red√©marrer le job Spark
docker exec spark-master pkill -f spark-submit
bash submit_consumer.sh
```

### Probl√®me: Producer stuck ou Maven timeout
**Solution:**
```powershell
# Augmenter le timeout Maven
cd producer
mvn exec:java@default -DskipTests -T 1C -X
```

---

## üíæ SAUVEGARDE DES DONN√âES

### Exporter les donn√©es de PostgreSQL
```powershell
docker exec postgres pg_dump -U kafka_user kafka_streaming > backup.sql
```

### Restaurer les donn√©es
```powershell
docker exec -i postgres psql -U kafka_user kafka_streaming < backup.sql
```

---

## üîê S√âCURIT√â (Production)

### Changer les passwords
```yaml
# docker-compose.yml
environment:
  POSTGRES_PASSWORD: votre_nouveau_password_securise
```

### Ajouter l'authentification Kafka
```yaml
# docker-compose.yml
environment:
  KAFKA_SECURITY_PROTOCOL: SASL_PLAINTEXT
  KAFKA_SASL_MECHANISM: PLAIN
```

---

## üìà PERFORMANCE TUNING

### Augmenter la m√©moire Spark
```yaml
# docker-compose.yml - spark-master
environment:
  SPARK_DRIVER_MEMORY: 4g
  SPARK_EXECUTOR_MEMORY: 2g
```

### Augmenter le batch size
```java
// KafkaProducerApp.java
private static final int BATCH_SIZE = 500;  // De 100 √† 500
```

### Augmenter les partitions Kafka
```bash
# create_topic.sh
--partitions 4  # De 1 √† 4
```

---

## üéØ CHECKLIST COMPL√àTE

- [ ] Docker-compose up -d ex√©cut√©
- [ ] Tous les 7 services sont actifs (docker ps)
- [ ] Topic Kafka cr√©√©
- [ ] Producer compil√© (mvn clean package)
- [ ] Spark Streaming job soumis (bash submit_consumer.sh)
- [ ] Producer ex√©cut√© (mvn exec:java@default)
- [ ] Kafka UI accessible et voir les messages
- [ ] Spark Master UI accessible
- [ ] PostgreSQL contient 506 records
- [ ] PgAdmin accessible et connect√©

---

## üöÄ QUICK START ONE-LINER (PowerShell)

```powershell
docker-compose up -d; Start-Sleep -Seconds 20; docker exec kafka kafka-topics --create --topic housing-data --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists; cd producer; mvn clean package
```

Puis dans 3 terminals s√©par√©s:
```powershell
# Terminal 1
bash submit_consumer.sh

# Terminal 2
cd producer; mvn exec:java@default

# Terminal 3
Start-Sleep -Seconds 30; docker exec postgres psql -U kafka_user -d kafka_streaming -c "SELECT COUNT(*) FROM housing;"
```

